# MLT
The home tasks from the MLT class of the ITMO Big Data and Machine Learning Master's program
| Task | The goal | Stack |
| ----------- | ----------- | ----------- |
| Task 1 - [Decision trees and random forests](https://github.com/olpeshiki/MLT/blob/6f5b328e18a6ad1688a7ea3899bc4b11a5b37d2d/Task1_Olga_J4133c.ipynb) | The goal is to train classifiers - decision trees and random forests, to predict and to then analize the results|`pandas`, `numpy`,`sklearn`,`matplotlib` |
| Task 2 - [Natural Language Processing](https://github.com/olpeshiki/MLT/blob/e9304e543f2b7b6ea391822e34e54dfeae6665df/Task2_Olga_J4133c.ipynb) |The goal is to preprocess the text of Alice in Wonderland by Lewis Carroll - removing stop words, lemmatizing etc. As well as to fing top 10 verbs and top 10 most used words per chpter | `nltk` , `sklearn`, `spacy`|
| Task 3 - [Optimization methods](https://github.com/olpeshiki/MLT/blob/ce73759b155a9cf79de6cbb25a19d00bdc927743/Task3_Olga_J4133c.ipynb) | The goals in to implement the Gradient Descent with momentum and Adam Optimization functions and compare their performance | `numpy`,`matplotlib`, `seaborn`|
| Task 4 - [Regression method as a Neural Network](https://github.com/olpeshiki/MLT/blob/f41ceccf84e2db88a9e0ac1133f04a2351b0c49d/Task4_Olga_J4133c.ipynb) | The goal is to apply the logistic regression method using the three optimization methods - Gradientt Descent, Stochastic Gradient Descent and Adam to predict the biological response of a molecule | `pandas`, `numpy`,`scipy`,`sklearn`,`matplotlib`|
| Task 5 - [CNN and Transfer Learning](https://github.com/olpeshiki/MLT/blob/e9304e543f2b7b6ea391822e34e54dfeae6665df/Task5_Olga_J4133c.ipynb) | The goal is to learn how to distinguish dogs from cats by solving the binary classification problem | `keras`, `tensorflow`,`matplotlib`|
